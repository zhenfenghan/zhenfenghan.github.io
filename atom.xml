<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Will的博客</title>
  <icon>https://www.gravatar.com/avatar/7c8ece4bce7f5caa1330b25979808087</icon>
  <subtitle>愿意探索生活的更多可能性</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zhenfenghan.github.io/"/>
  <updated>2018-04-24T02:03:09.468Z</updated>
  <id>https://zhenfenghan.github.io/</id>
  
  <author>
    <name>Will Han</name>
    <email>zhfhan@foxmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>t-SNE（t-分布随机邻域嵌入）算法</title>
    <link href="https://zhenfenghan.github.io/2018/04/23/t-SNE%E7%AE%97%E6%B3%95/"/>
    <id>https://zhenfenghan.github.io/2018/04/23/t-SNE算法/</id>
    <published>2018-04-23T01:01:11.000Z</published>
    <updated>2018-04-24T02:03:09.468Z</updated>
    
    <content type="html"><![CDATA[<p><strong>t-SNE(t-distributed stochastic neighbor embedding) </strong> 是用于<strong>降维</strong>的一种机器学习算法，是由 Laurens van der Maaten 和 Geoffrey Hinton在08年提出来。此外，t-SNE 是一种非线性降维算法，非常适用于高维数据降维到2维或者3维，进行可视化。</p><p>t-SNE是由SNE(Stochastic Neighbor Embedding, SNE; Hinton and Roweis, 2002)发展而来。我们先介绍SNE的基本原理，之后再扩展到t-SNE。最后再看一下t-SNE的实现以及一些优化。</p><h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><ul><li><a href="file:///D:/Program%20Files/HexoEditor/resources/app.asar/views/main/index.html#1sne" target="_blank" rel="noopener">1.SNE</a><ul><li><a href="file:///D:/Program%20Files/HexoEditor/resources/app.asar/views/main/index.html#11基本原理" target="_blank" rel="noopener">1.1基本原理</a></li><li><a href="file:///D:/Program%20Files/HexoEditor/resources/app.asar/views/main/index.html#12-sne原理推导" target="_blank" rel="noopener">1.2 SNE原理推导</a></li></ul></li><li><a href="file:///D:/Program%20Files/HexoEditor/resources/app.asar/views/main/index.html#2t-sne" target="_blank" rel="noopener">2.t-SNE</a><ul><li><a href="file:///D:/Program%20Files/HexoEditor/resources/app.asar/views/main/index.html#21-symmetric-sne" target="_blank" rel="noopener">2.1 Symmetric SNE</a></li><li><a href="file:///D:/Program%20Files/HexoEditor/resources/app.asar/views/main/index.html#22-crowding问题" target="_blank" rel="noopener">2.2 Crowding问题</a></li><li><a href="file:///D:/Program%20Files/HexoEditor/resources/app.asar/views/main/index.html#23-t-sne" target="_blank" rel="noopener">2.3 t-SNE</a></li><li><a href="file:///D:/Program%20Files/HexoEditor/resources/app.asar/views/main/index.html#24-算法过程" target="_blank" rel="noopener">2.4 算法过程</a></li><li><a href="file:///D:/Program%20Files/HexoEditor/resources/app.asar/views/main/index.html#25-不足" target="_blank" rel="noopener">2.5 不足</a></li></ul></li><li><a href="file:///D:/Program%20Files/HexoEditor/resources/app.asar/views/main/index.html#3变种" target="_blank" rel="noopener">3.变种</a></li><li><a href="file:///D:/Program%20Files/HexoEditor/resources/app.asar/views/main/index.html#4参考文档" target="_blank" rel="noopener">4.参考文档</a></li><li><a href="file:///D:/Program%20Files/HexoEditor/resources/app.asar/views/main/index.html#5-代码" target="_blank" rel="noopener">5. 代码</a></li></ul><a id="more"></a><h3 id="1-SNE"><a href="#1-SNE" class="headerlink" title="1.SNE"></a>1.SNE</h3><h4 id="1-1基本原理"><a href="#1-1基本原理" class="headerlink" title="1.1基本原理"></a>1.1基本原理</h4><p>SNE是通过仿射(affinitie)变换将数据点映射到概率分布上，主要包括两个步骤：</p><ul><li>SNE构建一个高维对象之间的概率分布，使得相似的对象有更高的概率被选择，而不相似的对象有较低的概率被选择。</li><li>SNE在低维空间里在构建这些点的概率分布，使得这两个概率分布之间尽可能的相似。</li></ul><p>我们看到t-SNE模型是非监督的降维，他跟kmeans等不同，他不能通过训练得到一些东西之后再用于其它数据（比如kmeans可以通过训练得到k个点，再用于其它数据集，而t-SNE只能单独的对数据做操作，也就是说他只有fit_transform，而没有fit操作）</p><h4 id="1-2-SNE原理推导"><a href="#1-2-SNE原理推导" class="headerlink" title="1.2 SNE原理推导"></a>1.2 SNE原理推导</h4><p>SNE是先<strong>将欧几里得距离转换为条件概率来表达点与点之间的相似度</strong>。具体来说，给定一个N个高维的数据 $ x_1,…,x_N  $（注意N不是维度）, t-SNE首先是计算概率$ p_{ij} $，正比于$ x_i $ 和$ x_j $ 之间的相似度（这种概率是我们自主构建的），即：</p><p>$$ {p_ {j \mid i} = \frac{\exp(- \mid \mid x_i -x_j \mid \mid ^2 / (2 \sigma^2_i ))} {\sum_{k \neq i} \exp(- \mid \mid x_i - x_k \mid \mid ^2 / (2 \sigma^2_i))}} $$</p><p>这里的有一个参数是$\sigma_i $，对于不同的点$x_i$ 取值不一样，后续会讨论如何设置。此外设置$ p_{x \mid x}=0 $,因为我们关注的是两两之间的相似度。</p><p>那对于低维度下的$ y_i $，我们可以指定高斯分布为方差为$ \frac{1}{\sqrt{2}} $，因此它们之间的相似度如下:</p><p>$$<br>{q_ {j \mid i} = \frac{\exp(- \mid \mid x_i -x_j \mid \mid ^2)} {\sum_{k \neq i} \exp(- \mid \mid x_i - x_k \mid \mid ^2)}} $$</p><p>同样，设定$ q_{i \mid i} = 0 $.</p><p>如果降维的效果比较好，局部特征保留完整，那么 $ p_{i \mid j} = q_{i \mid j} $, 因此我们优化两个分布之间的距离–KL散度(Kullback-Leibler divergences)，那么目标函数(cost function)如下:</p><p>$$<br>C = \sum_i KL(P_i \mid \mid Q_i) = \sum_i \sum_j p_{j \mid i} \log \frac{p_{j \mid i}}{q_{j \mid i}} $$</p><p>这里的$ P_i $表示了给定点$ x_i $下，其他所有数据点的条件概率分布。需要注意的是<strong>KL散度具有不对称性</strong>，在低维映射中不同的距离对应的惩罚权重是不同的，具体来说：距离较远的两个点来表达距离较近的两个点会产生更大的cost，相反，用较近的两个点来表达较远的两个点产生的cost相对较小(注意：类似于回归容易受异常值影响，但效果相反)。即用较小的 $ q_{j \mid i}=0.2 $ 来建模较大的 $ p_{j \mid i}=0.8$ , $cost= p \log(\frac{p}{q}) =1.11$,同样用较大的$ q_{j \mid i}=0.8 $ 来建模较大的$ p_{j \mid i}=0.2 , cost=-0.277$, 因此，<strong>SNE会倾向于保留数据中的局部特征</strong>。</p><blockquote><p>思考:了解了基本思路之后，你会怎么选择$ \sigma $，固定初始化?</p></blockquote><p>下面我们开始正式的推导SNE。首先不同的点具有不同的$ \sigma_i$ ，$ P_i $的熵(entropy)会随着$\sigma_i$ 的增加而增加。SNE使用困惑度(<a href="https://en.wikipedia.org/wiki/Perplexity" target="_blank" rel="noopener">perplexity</a>)的概念，用二分搜索的方式来寻找一个最佳的$\sigma$  。其中困惑度指:</p><p>$$<br>Perp(P_i) = 2^{H(P_i)}<br>$$<br>这里的$ H(P_i)$ 是$ P_i $ 的熵，即:</p><p>$$<br>H(P_i) = -\sum_j p_{j \mid i} \log_2 p_{j \mid i}<br>$$<br>困惑度可以解释为一个点附近的有效近邻点个数。<strong>SNE对困惑度的调整比较有鲁棒性，通常选择5-50之间</strong>，给定之后，使用二分搜索的方式寻找合适的$\sigma$</p><p>那么核心问题是如何求解梯度了,目标函数等价于$\sum \sum - p log(q)$ 这个式子与softmax非常的类似，我们知道softmax的目标函数是$ \sum -y \log p $，对应的梯度是$ y - p $  (注：这里的softmax中y表示label，p表示预估值)。 同样我们可以推导SNE的目标函数中的i在j下的条件概率情况的梯度是$2(p_{i \mid j}-q_{i \mid j})(y_i-y_j) $， 同样j在i下的条件概率的梯度是$2(p_{j \mid i}-q_{j \mid i})(y_i-y_j)$ , 最后得到完整的梯度公式如下:<br>$$<br>\frac{\delta C}{\delta y_i} = 2 \sum_j (p_{j \mid i} - q_{j \mid i} + p_{i \mid j} - q_{i \mid j})(y_i - y_j)<br>$$<br>在初始化中，可以用较小的$\sigma$ 下的高斯分布来进行初始化。为了加速优化过程和避免陷入局部最优解，梯度中需要使用一个相对较大的动量(momentum)。即参数更新中除了当前的梯度，还要引入之前的梯度累加的指数衰减项，如下:</p><p>$$<br>Y^{(t)} = Y^{(t-1)} + \eta \frac{\delta C}{\delta Y} + \alpha(t)(Y^{(t-1)} - Y^{(t-2)})<br>$$<br>这里的$ Y^{(t)} $表示迭代t次的解，$ \eta $表示学习速率,$\alpha(t) $表示迭代t次的动量。</p><p>此外，在初始优化的阶段，每次迭代中可以引入一些高斯噪声，之后像模拟退火一样逐渐减小该噪声，可以用来避免陷入局部最优解。因此，SNE在选择高斯噪声，以及学习速率，什么时候开始衰减，动量选择等等超参数上，需要跑多次优化才可以。</p><blockquote><p>思考:SNE有哪些不足？ 面对SNE的不足，你会做什么改进？  </p></blockquote><h3 id="2-t-SNE"><a href="#2-t-SNE" class="headerlink" title="2.t-SNE"></a>2.t-SNE</h3><p>尽管SNE提供了很好的可视化方法，但是他很难优化，而且存在”crowding problem”(拥挤问题)。后续中，Hinton等人又提出了t-SNE的方法。与SNE不同，主要如下:</p><ul><li>使用对称版的SNE，简化梯度公式</li><li>低维空间下，使用t分布替代高斯分布表达两点之间的相似度</li></ul><p>t-SNE在低维空间下使用更重长尾分布的t分布来避免crowding问题和优化问题。在这里，首先介绍一下对称版的SNE，之后介绍crowding问题，之后再介绍t-SNE。</p><h4 id="2-1-Symmetric-SNE"><a href="#2-1-Symmetric-SNE" class="headerlink" title="2.1 Symmetric SNE"></a>2.1 Symmetric SNE</h4><p>优化$ p_{i \mid j}$ 和$ q_{i \mid j} $的KL散度的一种替换思路是，使用联合概率分布来替换条件概率分布，即P是高维空间里各个点的联合概率分布，Q是低维空间下的，目标函数为:</p><p>$$<br>C = KL(P \mid \mid Q) = \sum_i \sum_j p_{i,j} \log \frac{p_{ij}}{q_{ij}}<br>$$<br>这里的$ p_{ii}$ ,$q_{ii}$ 为0，我们将这种SNE称之为symmetric SNE(对称SNE)，因为他假设了对于任意i,$ p_{ij} = p_{ji}$  , $q_{ij} = q_{ji} $，因此概率分布可以改写为:</p><p>$$<br>p_{ij} = \frac{\exp(- \mid \mid x_i - x_j \mid \mid ^2 / 2\sigma^2)}{\sum_{k \neq l} \exp(- \mid \mid x_k-x_l \mid \mid ^2 / 2\sigma^2)} \ \ \ \ q_{ij} = \frac{\exp(- \mid \mid y_i - y_j \mid \mid ^2)}{\sum_{k \neq l} \exp(- \mid \mid y_k-y_l \mid \mid ^2)}<br>$$<br>这种表达方式，使得整体简洁了很多。但是会引入<strong>异常值</strong>的问题。比如$ x_i $ 是异常值，那么$ \mid \mid x_i - x_j \mid \mid ^2 $会很大，对应的所有的j,$p_{ij} $都会很小(之前是仅在$ x_i $下很小)，导致低维映射下的$ y_i$ 对cost影响很小。</p><blockquote><p>思考: 对于异常值，你会做什么改进 ？$ p_i$ 表示什么 ？</p></blockquote><p>为了解决这个问题，我们将联合概率分布定义修正为: $ p_{ij} = \frac{p_{i \mid j} + p_{j \mid i}}{2}$ , 这保证了$\sum_j p_{ij} \gt \frac{1}{2n} $, 使得每个点对于cost都会有一定的贡献。对称SNE的最大优点是梯度计算变得简单了，如下:</p><p>$$<br>\frac{\delta C}{\delta y_i} = 4 \sum_j (p_{ij} - q_{ij})(y_i - y_j)<br>$$<br>实验中，发现对称SNE能够产生和SNE一样好的结果，有时甚至略好一点。</p><h4 id="2-2-Crowding问题"><a href="#2-2-Crowding问题" class="headerlink" title="2.2 Crowding问题"></a>2.2 Crowding问题</h4><p>拥挤问题就是说各个簇聚集在一起，无法区分。比如有一种情况，高维度数据在降维到10维下，可以有很好的表达，但是降维到两维后无法得到可信映射，比如降维如10维中有11个点之间两两等距离的，在二维下就无法得到可信的映射结果(最多3个点)。 进一步的说明，假设一个以数据点$x_i$ 为中心，半径为r的m维球(三维空间就是球)，其体积是按$ r^m $增长的，假设数据点是在m维球中均匀分布的，我们来看看其他数据点与$x_i$ 的距离随维度增大而产生的变化。</p><p><img src="http://p6ux47i4n.bkt.clouddn.com/sne_crowding.png" alt="sne_crowding.png"></p><p>从上图可以看到，随着维度的增大，大部分数据点都聚集在m维球的表面附近，与点xixixi”&gt; x_i 的距离分布极不均衡。如果直接将这种距离关系保留到低维，就会出现拥挤问题。</p><blockquote><p>怎么解决crowding问题呢？</p></blockquote><p><em>Cook et al.(2007)</em> 提出一种 slight repulsion 的方式，在基线概率分布(uniform background)中引入一个较小的混合因子$\rho$ ,这样$q_{ij}$ 就永远不会小于$ \frac{2 \rho}{n(n-1)} $(因为一共了n(n-1)个pairs)，这样在高维空间中比较远的两个点之间的$q_{ij}$ 总是会比$p_{ij}$ 大一点。这种称之为UNI-SNE，效果通常比标准的SNE要好。优化UNI-SNE的方法是先让$\rho $为0，使用标准的SNE优化，之后用模拟退火的方法的时候，再慢慢增加$\rho $. 直接优化UNI-SNE是不行的(即一开始$\rho$ 不为0)，因为距离较远的两个点基本是一样的$q_{ij} $(等于基线分布), 即使$p_{ij} $很大，一些距离变化很难在$ q_{ij}$ 中产生作用。也就是说优化中刚开始距离较远的两个聚类点，后续就无法再把他们拉近了。</p><h4 id="2-3-t-SNE"><a href="#2-3-t-SNE" class="headerlink" title="2.3 t-SNE"></a>2.3 t-SNE</h4><p>对称SNE实际上在高维度下 另外一种减轻”拥挤问题”的方法：在高维空间下，在高维空间下我们使用高斯分布将距离转换为概率分布，在低维空间下，我们使用更加偏重长尾分布的方式来将距离转换为概率分布，使得高维度下中低等的距离在映射后能够有一个较大的距离。</p><p><img src="http://p6ux47i4n.bkt.clouddn.com/norm_t_dict.png" alt="norm_t_dict.png"></p><p>我们对比一下高斯分布和t分布(如上图,code见probability/distribution.md), t分布受异常值影响更小，拟合结果更为合理，较好的捕获了数据的整体特征。</p><p>使用了t分布之后的q变化，如下:</p><p>$$<br>q_{ij} = \frac{(1 + \mid \mid y_i -y_j \mid \mid ^2)^{-1}}{\sum_{k \neq l} (1 + \mid \mid y_i -y_j \mid \mid ^2)^{-1}}<br>$$<br>此外，t分布是无限多个高斯分布的叠加，计算上不是指数的，会方便很多。优化的梯度如下:</p><p>$$<br>\frac{\delta C}{\delta y_i} = 4 \sum_j(p_{ij}-q_{ij})(y_i-y_j)(1+ \mid \mid y_i-y_j \mid \mid ^2)^{-1}<br>$$<br><img src="http://p6ux47i4n.bkt.clouddn.com/sne_norm_t_dist_cost.png" alt="ne_norm_t_dist_cost.png"></p><p>t-sne的有效性，也可以从上图中看到：横轴表示距离，纵轴表示相似度, 可以看到，对于较大相似度的点，t分布在低维空间中的距离需要稍小一点；而对于低相似度的点，t分布在低维空间中的距离需要更远。这恰好满足了我们的需求，即同一簇内的点(距离较近)聚合的更紧密，不同簇之间的点(距离较远)更加疏远。</p><p>总结一下，t-SNE的梯度更新有两大优势：</p><ul><li>对于不相似的点，用一个较小的距离会产生较大的梯度来让这些点排斥开来。</li><li>这种排斥又不会无限大(梯度中分母)，避免不相似的点距离太远。</li></ul><h4 id="2-4-算法过程"><a href="#2-4-算法过程" class="headerlink" title="2.4 算法过程"></a>2.4 算法过程</h4><p>算法详细过程如下：</p><ul><li>Data: $X = {x_1, … , x_n}$</li><li>计算cost function的参数：困惑度Perp</li><li>优化参数: 设置迭代次数T， 学习速率$\eta$ , 动量$\alpha(t)$</li><li>目标结果是低维数据表示 $ Y^T = {y_1, … , y_n}$</li><li>开始优化<ul><li>计算在给定Perp下的条件概率$ p_{j \mid i}$ (参见上面公式)</li><li>令 $ p_{ij} = \frac{p_{j \mid i} + p_{i \mid j}}{2n}$</li><li>用$N(0, 10^{-4}I) $随机初始化 Y</li><li>迭代，从 t = 1 到 T， 做如下操作:<ul><li>计算低维度下的$ q_{ij} $(参见上面的公式)</li><li>计算梯度（参见上面的公式）</li><li>更新 $ Y^{t} = Y^{t-1} + \eta \frac{dC}{dY} + \alpha(t)(Y^{t-1} - Y^{t-2})$</li></ul></li><li>结束</li></ul></li><li>结束</li></ul><p>优化过程中可以尝试的两个trick:</p><ul><li>提前压缩(early compression):开始初始化的时候，各个点要离得近一点。这样小的距离，方便各个聚类中心的移动。可以通过引入L2正则项(距离的平方和)来实现。</li><li>提前夸大(early exaggeration)：在开始优化阶段，$ p_{ij} $乘以一个大于1的数进行扩大，来避免因为$q_{ij} $太小导致优化太慢的问题。比如前50次迭代，$p_{ij} $乘以4</li></ul><p>优化的过程动态图如下：</p><p><img src="http://p6ux47i4n.bkt.clouddn.com/t-sne_optimise.gif" alt="t-sne_optimise.gif"></p><h4 id="2-5-不足"><a href="#2-5-不足" class="headerlink" title="2.5 不足"></a>2.5 不足</h4><p>主要不足有四个:</p><ul><li>主要用于可视化，很难用于其他目的。比如测试集合降维，因为他没有显式的预估部分，不能在测试集合直接降维；比如降维到10维，因为t分布偏重长尾，1个自由度的t分布很难保存好局部特征，可能需要设置成更高的自由度。</li><li>t-SNE倾向于保存局部特征，对于本征维数(intrinsic dimensionality)本身就很高的数据集，是不可能完整的映射到2-3维的空间</li><li>t-SNE没有唯一最优解，且没有预估部分。如果想要做预估，可以考虑降维之后，再构建一个回归方程之类的模型去做。但是要注意，t-sne中距离本身是没有意义，都是概率分布问题。</li><li>训练太慢。有很多基于树的算法在t-sne上做一些改进</li></ul><h3 id="3-变种"><a href="#3-变种" class="headerlink" title="3.变种"></a>3.变种</h3><p>后续有机会补充。</p><ul><li><strong>multiple maps of t-SNE</strong></li><li><strong>parametric t-SNE</strong></li><li><strong>Visualizing Large-scale and High-dimensional Data</strong></li></ul><h3 id="4-参考文档"><a href="#4-参考文档" class="headerlink" title="4.参考文档"></a>4.参考文档</h3><ul><li><em>Maaten, L., &amp; Hinton, G. (2008). Visualizing data using t-SNE. Journal of Machine Learning Research.</em></li></ul><h3 id="5-代码"><a href="#5-代码" class="headerlink" title="5. 代码"></a>5. 代码</h3><p><strong>文中的插图绘制:</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from numpy.linalg import norm</span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line"></span><br><span class="line">def sne_crowding():</span><br><span class="line">    npoints = 1000 <span class="comment"># 抽取1000个m维球内均匀分布的点</span></span><br><span class="line">    plt.figure(figsize=(20, 5))</span><br><span class="line">    <span class="keyword">for</span> i, m <span class="keyword">in</span> enumerate((2, 3, 5, 8)):</span><br><span class="line">        <span class="comment"># 这里模拟m维球中的均匀分布用到了拒绝采样，</span></span><br><span class="line">        <span class="comment"># 即先生成m维立方中的均匀分布，再剔除m维球外部的点</span></span><br><span class="line">        accepts = []</span><br><span class="line">        <span class="keyword">while</span> len(accepts) &lt; 1000:</span><br><span class="line">            points = np.random.rand(500, m)</span><br><span class="line">            accepts.extend([d <span class="keyword">for</span> d <span class="keyword">in</span> norm(points, axis=1)</span><br><span class="line">                            <span class="keyword">if</span> d &lt;= 1.0]) <span class="comment"># 拒绝采样</span></span><br><span class="line">        accepts = accepts[:npoints]</span><br><span class="line">        ax = plt.subplot(1, 4, i+1)</span><br><span class="line">        <span class="keyword">if</span> i == 0:</span><br><span class="line">            ax.set_ylabel(<span class="string">'count'</span>)</span><br><span class="line">        <span class="keyword">if</span> i == 2:</span><br><span class="line"></span><br><span class="line">            ax.set_xlabel(<span class="string">'distance'</span>)</span><br><span class="line">        ax.hist(accepts, bins=np.linspace(0., 1., 50))</span><br><span class="line">        ax.set_title(<span class="string">'m=%s'</span> %m)</span><br><span class="line">    plt.savefig(<span class="string">"./images/sne_crowding.png"</span>)</span><br><span class="line"></span><br><span class="line">    x = np.linspace(0, 4, 100)</span><br><span class="line">    ta = 1 / (1 + np.square(x))</span><br><span class="line">    tb = np.sum(ta) - 1</span><br><span class="line">    qa = np.exp(-np.square(x))</span><br><span class="line">    qb = np.sum(qa) - 1</span><br><span class="line"></span><br><span class="line">def sne_norm_t_dist_cost():</span><br><span class="line">    plt.figure(figsize=(8, 5))</span><br><span class="line">    plt.plot(qa/qb, c=<span class="string">"b"</span>, label=<span class="string">"normal-dist"</span>)</span><br><span class="line">    plt.plot(ta/tb, c=<span class="string">"g"</span>, label=<span class="string">"t-dist"</span>)</span><br><span class="line">    plt.plot((0, 20), (0.025, 0.025), <span class="string">'r--'</span>)</span><br><span class="line">    plt.text(10, 0.022, r<span class="string">'$q_&#123;ij&#125;$'</span>)</span><br><span class="line">    plt.text(20, 0.026, r<span class="string">'$p_&#123;ij&#125;$'</span>)</span><br><span class="line"></span><br><span class="line">    plt.plot((0, 55), (0.005, 0.005), <span class="string">'r--'</span>)</span><br><span class="line">    plt.text(36, 0.003, r<span class="string">'$q_&#123;ij&#125;$'</span>)</span><br><span class="line">    plt.text(55, 0.007, r<span class="string">'$p_&#123;ij&#125;$'</span>)</span><br><span class="line"></span><br><span class="line">    plt.title(<span class="string">"probability of distance"</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"distance"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"probability"</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.savefig(<span class="string">"./images/sne_norm_t_dist_cost.png"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    sne_crowding()</span><br><span class="line">    sne_norm_t_dist_cost()</span><br></pre></td></tr></table></figure></p><p><strong>t-sne的完整代码实现:</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">代码参考了作者Laurens van der Maaten的开放出的t-sne代码, 并没有用类进行实现,主要是优化了计算的实现</span></span><br><span class="line"><span class="string">'</span><span class="string">''</span></span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def cal_pairwise_dist(x):</span><br><span class="line">    <span class="string">''</span><span class="string">'计算pairwise 距离, x是matrix</span></span><br><span class="line"><span class="string">    (a-b)^2 = a^w + b^2 - 2*a*b</span></span><br><span class="line"><span class="string">    '</span><span class="string">''</span></span><br><span class="line">    sum_x = np.sum(np.square(x), 1)</span><br><span class="line">    dist = np.add(np.add(-2 * np.dot(x, x.T), sum_x).T, sum_x)</span><br><span class="line">    <span class="built_in">return</span> dist</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def cal_perplexity(dist, idx=0, beta=1.0):</span><br><span class="line">    <span class="string">''</span><span class="string">'计算perplexity, D是距离向量，</span></span><br><span class="line"><span class="string">    idx指dist中自己与自己距离的位置，beta是高斯分布参数</span></span><br><span class="line"><span class="string">    这里的perp仅计算了熵，方便计算</span></span><br><span class="line"><span class="string">    '</span><span class="string">''</span></span><br><span class="line">    prob = np.exp(-dist * beta)</span><br><span class="line">    <span class="comment"># 设置自身prob为0</span></span><br><span class="line">    prob[idx] = 0</span><br><span class="line">    sum_prob = np.sum(prob)</span><br><span class="line">    perp = np.log(sum_prob) + beta * np.sum(dist * prob) / sum_prob</span><br><span class="line">    prob /= sum_prob</span><br><span class="line">    <span class="built_in">return</span> perp, prob</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def seach_prob(x, tol=1e-5, perplexity=30.0):</span><br><span class="line">    <span class="string">''</span><span class="string">'二分搜索寻找beta,并计算pairwise的prob</span></span><br><span class="line"><span class="string">    '</span><span class="string">''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"Computing pairwise distances..."</span>)</span><br><span class="line">    (n, d) = x.shape</span><br><span class="line">    dist = cal_pairwise_dist(x)</span><br><span class="line">    pair_prob = np.zeros((n, n))</span><br><span class="line">    beta = np.ones((n, 1))</span><br><span class="line">    <span class="comment"># 取log，方便后续计算</span></span><br><span class="line">    base_perp = np.log(perplexity)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="keyword">if</span> i % 500 == 0:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"Computing pair_prob for point %s of %s ..."</span> %(i,n))</span><br><span class="line"></span><br><span class="line">        betamin = -np.inf</span><br><span class="line">        betamax = np.inf</span><br><span class="line">        perp, this_prob = cal_perplexity(dist[i], i, beta[i])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 二分搜索,寻找最佳sigma下的prob</span></span><br><span class="line">        perp_diff = perp - base_perp</span><br><span class="line">        tries = 0</span><br><span class="line">        <span class="keyword">while</span> np.abs(perp_diff) &gt; tol and tries &lt; 50:</span><br><span class="line">            <span class="keyword">if</span> perp_diff &gt; 0:</span><br><span class="line">                betamin = beta[i].copy()</span><br><span class="line">                <span class="keyword">if</span> betamax == np.inf or betamax == -np.inf:</span><br><span class="line">                    beta[i] = beta[i] * 2</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    beta[i] = (beta[i] + betamax) / 2</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                betamax = beta[i].copy()</span><br><span class="line">                <span class="keyword">if</span> betamin == np.inf or betamin == -np.inf:</span><br><span class="line">                    beta[i] = beta[i] / 2</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    beta[i] = (beta[i] + betamin) / 2</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 更新perb,prob值</span></span><br><span class="line">            perp, this_prob = cal_perplexity(dist[i], i, beta[i])</span><br><span class="line">            perp_diff = perp - base_perp</span><br><span class="line">            tries = tries + 1</span><br><span class="line">        <span class="comment"># 记录prob值</span></span><br><span class="line">        pair_prob[i,] = this_prob</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"Mean value of sigma: "</span>, np.mean(np.sqrt(1 / beta)))</span><br><span class="line">    <span class="built_in">return</span> pair_prob</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def pca(x, no_dims = 50):</span><br><span class="line">    <span class="string">''</span><span class="string">' PCA算法</span></span><br><span class="line"><span class="string">    使用PCA先进行预降维</span></span><br><span class="line"><span class="string">    '</span><span class="string">''</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"Preprocessing the data using PCA..."</span>)</span><br><span class="line">    (n, d) = x.shape</span><br><span class="line">    x = x - np.tile(np.mean(x, 0), (n, 1))</span><br><span class="line">    l, M = np.linalg.eig(np.dot(x.T, x))</span><br><span class="line">    y = np.dot(x, M[:,0:no_dims])</span><br><span class="line">    <span class="built_in">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def tsne(x, no_dims=2, initial_dims=50, perplexity=30.0, max_iter=1000):</span><br><span class="line">    <span class="string">""</span><span class="string">"Runs t-SNE on the dataset in the NxD array x</span></span><br><span class="line"><span class="string">    to reduce its dimensionality to no_dims dimensions.</span></span><br><span class="line"><span class="string">    The syntaxis of the function is Y = tsne.tsne(x, no_dims, perplexity),</span></span><br><span class="line"><span class="string">    where x is an NxD NumPy array.</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Check inputs</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(no_dims, <span class="built_in">float</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"Error: array x should have type float."</span>)</span><br><span class="line">        <span class="built_in">return</span> -1</span><br><span class="line">    <span class="keyword">if</span> round(no_dims) != no_dims:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"Error: number of dimensions should be an integer."</span>)</span><br><span class="line">        <span class="built_in">return</span> -1</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化参数和变量</span></span><br><span class="line">    x = pca(x, initial_dims).real</span><br><span class="line">    (n, d) = x.shape</span><br><span class="line">    initial_momentum = 0.5</span><br><span class="line">    final_momentum = 0.8</span><br><span class="line">    eta = 500</span><br><span class="line">    min_gain = 0.01</span><br><span class="line">    y = np.random.randn(n, no_dims)</span><br><span class="line">    dy = np.zeros((n, no_dims))</span><br><span class="line">    iy = np.zeros((n, no_dims))</span><br><span class="line">    gains = np.ones((n, no_dims))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对称化</span></span><br><span class="line">    P = seach_prob(x, 1e-5, perplexity)</span><br><span class="line">    P = P + np.transpose(P)</span><br><span class="line">    P = P / np.sum(P)</span><br><span class="line">    <span class="comment"># early exaggeration</span></span><br><span class="line">    P = P * 4</span><br><span class="line">    P = np.maximum(P, 1e-12)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Run iterations</span></span><br><span class="line">    <span class="keyword">for</span> iter <span class="keyword">in</span> range(max_iter):</span><br><span class="line">        <span class="comment"># Compute pairwise affinities</span></span><br><span class="line">        sum_y = np.sum(np.square(y), 1)</span><br><span class="line">        num = 1 / (1 + np.add(np.add(-2 * np.dot(y, y.T), sum_y).T, sum_y))</span><br><span class="line">        num[range(n), range(n)] = 0</span><br><span class="line">        Q = num / np.sum(num)</span><br><span class="line">        Q = np.maximum(Q, 1e-12)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute gradient</span></span><br><span class="line">        PQ = P - Q</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            dy[i,:] = np.sum(np.tile(PQ[:,i] * num[:,i], (no_dims, 1)).T * (y[i,:] - y), 0)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Perform the update</span></span><br><span class="line">        <span class="keyword">if</span> iter &lt; 20:</span><br><span class="line">            momentum = initial_momentum</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            momentum = final_momentum</span><br><span class="line">        gains = (gains + 0.2) * ((dy &gt; 0) != (iy &gt; 0)) + (gains * 0.8) * ((dy &gt; 0) == (iy &gt; 0))</span><br><span class="line">        gains[gains &lt; min_gain] = min_gain</span><br><span class="line">        iy = momentum * iy - eta * (gains * dy)</span><br><span class="line">        y = y + iy</span><br><span class="line">        y = y - np.tile(np.mean(y, 0), (n, 1))</span><br><span class="line">        <span class="comment"># Compute current value of cost function</span></span><br><span class="line">        <span class="keyword">if</span> (iter + 1) % 100 == 0:</span><br><span class="line">            <span class="keyword">if</span> iter &gt; 100:</span><br><span class="line">                C = np.sum(P * np.log(P / Q))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                C = np.sum( P/4 * np.log( P/4 / Q))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"Iteration "</span>, (iter + 1), <span class="string">": error is "</span>, C)</span><br><span class="line">        <span class="comment"># Stop lying about P-values</span></span><br><span class="line">        <span class="keyword">if</span> iter == 100:</span><br><span class="line">            P = P / 4</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"finished training!"</span>)</span><br><span class="line">    <span class="built_in">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># Run Y = tsne.tsne(X, no_dims, perplexity) to perform t-SNE on your dataset.</span></span><br><span class="line">    X = np.loadtxt(<span class="string">"mnist2500_X.txt"</span>)</span><br><span class="line">    labels = np.loadtxt(<span class="string">"mnist2500_labels.txt"</span>)</span><br><span class="line">    Y = tsne(X, 2, 50, 20.0)</span><br><span class="line">    from matplotlib import pyplot as plt</span><br><span class="line">    plt.scatter(Y[:,0], Y[:,1], 20, labels)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p><p align="right"><em>转自 ：<a href="http://www.datakit.cn/blog/2017/02/05/t_sne_full.html" target="_blank" rel="noopener">http://www.datakit.cn/blog/2017/02/05/t_sne_full.html</a></em></p>  ]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;t-SNE(t-distributed stochastic neighbor embedding) &lt;/strong&gt; 是用于&lt;strong&gt;降维&lt;/strong&gt;的一种机器学习算法，是由 Laurens van der Maaten 和 Geoffrey Hinton在08年提出来。此外，t-SNE 是一种非线性降维算法，非常适用于高维数据降维到2维或者3维，进行可视化。&lt;/p&gt;
&lt;p&gt;t-SNE是由SNE(Stochastic Neighbor Embedding, SNE; Hinton and Roweis, 2002)发展而来。我们先介绍SNE的基本原理，之后再扩展到t-SNE。最后再看一下t-SNE的实现以及一些优化。&lt;/p&gt;
&lt;h3 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;file:///D:/Program%20Files/HexoEditor/resources/app.asar/views/main/index.html#1sne&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;1.SNE&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;file:///D:/Program%20Files/HexoEditor/resources/app.asar/views/main/index.html#11基本原理&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;1.1基本原理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;file:///D:/Program%20Files/HexoEditor/resources/app.asar/views/main/index.html#12-sne原理推导&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;1.2 SNE原理推导&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;file:///D:/Program%20Files/HexoEditor/resources/app.asar/views/main/index.html#2t-sne&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;2.t-SNE&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;file:///D:/Program%20Files/HexoEditor/resources/app.asar/views/main/index.html#21-symmetric-sne&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;2.1 Symmetric SNE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;file:///D:/Program%20Files/HexoEditor/resources/app.asar/views/main/index.html#22-crowding问题&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;2.2 Crowding问题&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;file:///D:/Program%20Files/HexoEditor/resources/app.asar/views/main/index.html#23-t-sne&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;2.3 t-SNE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;file:///D:/Program%20Files/HexoEditor/resources/app.asar/views/main/index.html#24-算法过程&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;2.4 算法过程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;file:///D:/Program%20Files/HexoEditor/resources/app.asar/views/main/index.html#25-不足&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;2.5 不足&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;file:///D:/Program%20Files/HexoEditor/resources/app.asar/views/main/index.html#3变种&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;3.变种&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;file:///D:/Program%20Files/HexoEditor/resources/app.asar/views/main/index.html#4参考文档&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;4.参考文档&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;file:///D:/Program%20Files/HexoEditor/resources/app.asar/views/main/index.html#5-代码&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;5. 代码&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="t-SNE" scheme="https://zhenfenghan.github.io/tags/t-SNE/"/>
    
      <category term="降维" scheme="https://zhenfenghan.github.io/tags/%E9%99%8D%E7%BB%B4/"/>
    
      <category term="机器学习" scheme="https://zhenfenghan.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>PMCAFF 2018 第二期产品学徒</title>
    <link href="https://zhenfenghan.github.io/2018/04/19/pmcaff%E6%8A%A5%E5%91%8A/"/>
    <id>https://zhenfenghan.github.io/2018/04/19/pmcaff报告/</id>
    <published>2018-04-19T01:01:11.000Z</published>
    <updated>2018-04-23T11:47:45.366Z</updated>
    
    <content type="html"><![CDATA[<p><strong>PMCAFF 2018 第二期产品学徒</strong>  </p><h3 id="网易公开课产品体验报告"><a href="#网易公开课产品体验报告" class="headerlink" title="网易公开课产品体验报告"></a>网易公开课产品体验报告</h3><p><em>Slogan：让分享知识成为习惯</em></p><p>报告链接：<a href="https://www.pmcaff.com/discuss/answer/1179912791644224" target="_blank" rel="noopener">https://www.pmcaff.com/discuss/answer/1179912791644224</a>  </p><h3 id="超级课程表产品体验报告"><a href="#超级课程表产品体验报告" class="headerlink" title="超级课程表产品体验报告"></a>超级课程表产品体验报告</h3><p><em>Slogan：大学生必备App</em>    </p><p>报告链接：<a href="https://www.pmcaff.com/discuss/answer/1182835810293824" target="_blank" rel="noopener">https://www.pmcaff.com/discuss/answer/1182835810293824</a>  </p><h3 id="网易云课堂产品体验报告"><a href="#网易云课堂产品体验报告" class="headerlink" title="网易云课堂产品体验报告"></a>网易云课堂产品体验报告</h3><p><em>Slogan：知识与技能学习平台</em><br><a id="more"></a><br>报告链接：<a href="https://www.pmcaff.com/discuss/answer/1185947544815680" target="_blank" rel="noopener">https://www.pmcaff.com/discuss/answer/1185947544815680</a>  </p><h3 id="腾讯课堂产品体验报告"><a href="#腾讯课堂产品体验报告" class="headerlink" title="腾讯课堂产品体验报告"></a>腾讯课堂产品体验报告</h3><p><em>Slogan：专业的直播学习平台</em><br>报告链接：<br><a href="https://www.pmcaff.com/discuss/answer/1190143442625600" target="_blank" rel="noopener">https://www.pmcaff.com/discuss/answer/1190143442625600</a>  </p><h3 id="百词斩产品体验报告"><a href="#百词斩产品体验报告" class="headerlink" title="百词斩产品体验报告"></a>百词斩产品体验报告</h3><p><em>Slogan：背单词、学英语必备</em>  </p><p>报告链接： <a href="https://www.pmcaff.com/discuss/answer/1192963570789440" target="_blank" rel="noopener">https://www.pmcaff.com/discuss/answer/1192963570789440</a></p><h3 id="有道云笔记产品体验报告"><a href="#有道云笔记产品体验报告" class="headerlink" title="有道云笔记产品体验报告"></a>有道云笔记产品体验报告</h3><p><em>Slogan：支持扫描、语音速记等多种记录方式</em>  </p><p>报告链接：<a href="https://www.pmcaff.com/discuss/answer/1195899178910784" target="_blank" rel="noopener">https://www.pmcaff.com/discuss/answer/1195899178910784</a></p><h3 id="腾讯翻译君产品体验报告"><a href="#腾讯翻译君产品体验报告" class="headerlink" title="腾讯翻译君产品体验报告"></a>腾讯翻译君产品体验报告</h3><p><em>Slogan：英语词典和语音翻译</em>  </p><p>报告链接：<br><a href="https://www.pmcaff.com/discuss/answer/1200308012176448" target="_blank" rel="noopener">https://www.pmcaff.com/discuss/answer/1200308012176448</a></p><h3 id="有道翻译官产品体验报告"><a href="#有道翻译官产品体验报告" class="headerlink" title="有道翻译官产品体验报告"></a>有道翻译官产品体验报告</h3><p><em>Slogan：107种语言的随身翻译软件</em>  </p><p>报告链接：<a href="https://www.pmcaff.com/discuss/answer/1203118397412416" target="_blank" rel="noopener">https://www.pmcaff.com/discuss/answer/1203118397412416</a></p><h3 id="轻芒阅读产品体验报告"><a href="#轻芒阅读产品体验报告" class="headerlink" title="轻芒阅读产品体验报告"></a>轻芒阅读产品体验报告</h3><p><em>Slogan:在一个应用里刷你关心应用的内容</em>  </p><p>报告链接：<a href="https://www.pmcaff.com/discuss/answer/1206089174391872" target="_blank" rel="noopener">https://www.pmcaff.com/discuss/answer/1206089174391872</a></p><h3 id="喜马拉雅FM产品体验报告"><a href="#喜马拉雅FM产品体验报告" class="headerlink" title="喜马拉雅FM产品体验报告"></a>喜马拉雅FM产品体验报告</h3><p><em>Slogan：随时随地，听我想听</em>  </p><p>报告链接：<br><a href="https://www.pmcaff.com/discuss/answer/1216180173690944" target="_blank" rel="noopener">https://www.pmcaff.com/discuss/answer/1216180173690944</a></p><div align="center"><br><img src="http://p6ux47i4n.bkt.clouddn.com/pmcaff%20%E4%BC%98%E7%A7%80%E5%AD%A6%E5%91%98.jpg/500.400" alt="chanpinxuetu.jpg">　<br></div> ]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;PMCAFF 2018 第二期产品学徒&lt;/strong&gt;  &lt;/p&gt;
&lt;h3 id=&quot;网易公开课产品体验报告&quot;&gt;&lt;a href=&quot;#网易公开课产品体验报告&quot; class=&quot;headerlink&quot; title=&quot;网易公开课产品体验报告&quot;&gt;&lt;/a&gt;网易公开课产品体验报告&lt;/h3&gt;&lt;p&gt;&lt;em&gt;Slogan：让分享知识成为习惯&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;报告链接：&lt;a href=&quot;https://www.pmcaff.com/discuss/answer/1179912791644224&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.pmcaff.com/discuss/answer/1179912791644224&lt;/a&gt;  &lt;/p&gt;
&lt;h3 id=&quot;超级课程表产品体验报告&quot;&gt;&lt;a href=&quot;#超级课程表产品体验报告&quot; class=&quot;headerlink&quot; title=&quot;超级课程表产品体验报告&quot;&gt;&lt;/a&gt;超级课程表产品体验报告&lt;/h3&gt;&lt;p&gt;&lt;em&gt;Slogan：大学生必备App&lt;/em&gt;    &lt;/p&gt;
&lt;p&gt;报告链接：&lt;a href=&quot;https://www.pmcaff.com/discuss/answer/1182835810293824&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.pmcaff.com/discuss/answer/1182835810293824&lt;/a&gt;  &lt;/p&gt;
&lt;h3 id=&quot;网易云课堂产品体验报告&quot;&gt;&lt;a href=&quot;#网易云课堂产品体验报告&quot; class=&quot;headerlink&quot; title=&quot;网易云课堂产品体验报告&quot;&gt;&lt;/a&gt;网易云课堂产品体验报告&lt;/h3&gt;&lt;p&gt;&lt;em&gt;Slogan：知识与技能学习平台&lt;/em&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="PMCAFF" scheme="https://zhenfenghan.github.io/tags/PMCAFF/"/>
    
      <category term="产品" scheme="https://zhenfenghan.github.io/tags/%E4%BA%A7%E5%93%81/"/>
    
  </entry>
  
  <entry>
    <title>AI黑箱：用AI解释AI？</title>
    <link href="https://zhenfenghan.github.io/2018/04/15/AI%E9%BB%91%E7%AE%B1/"/>
    <id>https://zhenfenghan.github.io/2018/04/15/AI黑箱/</id>
    <published>2018-04-15T02:01:11.000Z</published>
    <updated>2018-04-19T02:27:52.779Z</updated>
    
    <content type="html"><![CDATA[<p><em>概要：AI算法对人类生活的影响越来越大，但它们内部的运作往往是不透明的，人们对这种技术的工作方式也愈加感到担忧。</em></p><div align="center"><br><img src="http://p6ux47i4n.bkt.clouddn.com/ai.jpg" alt="ai.jpg"><br></div><br>AI算法对人类生活的影响越来越大，但它们内部的运作往往是不透明的，人们对这种技术的工作方式也愈加感到担忧。MIT科技评论曾经发表一篇题为“人工智能中的黑暗秘密”的文章，警告说：“没有人真正知道先进的机器学习算法是怎样工作的，而这恐将成为一大隐忧。”由于这种不确定性和缺乏问责制，纽约大学AI Now Institute的一份报告建议负责刑事司法、医疗保健、社会福利和教育的公共机构不应该使用AI技术。<br><a id="more"></a><br>输入的数据和答案之间的不可观察的空间通常被称为“黑箱”（black box）——名称来自飞机上强制使用的飞行记录仪“黑匣子”（实际上是橙色的，而非黑色），并且经常在空难事故后用于向调查人员提供有关飞机当时运作情况的数据。在人工智能领域，这个术语描述了AI技术如何在“暗处”运作的景象：我们提供数据、模型和架构，然后计算机给出答案，同时以一种看似不可能的方式继续学习——显然对于我们人类来说，这太难理解了。<br><div align="center"><br><img src="http://p6ux47i4n.bkt.clouddn.com/black-box-input-output.png" alt="balckbox.png"><br></div><br>### 黑箱没有什么可怕的<br><br>在医疗领域，这个问题尤其被关注。AI被用于区分哪些皮肤病变是癌变，从血液中识别早期癌症，预测心脏疾病，确定人和动物的哪些化合物可以延长寿命，等等。但是，对黑箱的这些担忧是不必要的。AI的透明程度并不亚于医生一直以来的工作方式——在许多情况下，AI甚至是一种进步，它增强了医院的能力，对病人和整个医疗系统都有积极的作用。毕竟，对于新技术来说，AI的黑箱问题并不是一个新问题：人类智能本身就是一个黑箱，而且一直都是。<br><br>让我们来看一个人类医生做诊断的例子。病人可能会问医生她是如何做出诊断的，医生可能会说出一些她用来得出结论的数据。但她真的能够解释她是如何、以及为什么得出这个结论吗,她从哪些研究中得到哪些具体数据,她从所受的教育或导师那里得到了什么影响,她从自己以及同事的共同经验中得到哪些隐性知识，以及所有这些的结合如何引导她得出那个诊断?当然，她可能会说出引领她往某个特定方向走的某些指示，但这也会有猜测的成分，有跟随直觉的成分。即使没有，我们也仍然不知道有没有什么其他因素是她自己甚至没有意识到的。<br><br>如果使用AI进行同样的诊断，我们可以从该患者的所有可用信息中获取数据，以及在不同时间和从其他无数同类患者身上匿名收集的数据，用以做出最有力的基于证据的决策。这是一种与数据直接相关的诊断，而不是基于有限数据的人类直觉，或者相对少的局部患者的诊断经验总结。<br><div align="center"><br><img src="http://p6ux47i4n.bkt.clouddn.com/Main_Day1_Healthcare_MOV8_Patient_v06.gif" alt="aiinhealthy.gif"><br></div><br>但是，我们每天都必须在很多我们并不完全了解的领域做决策——并且通常都非常成功——从预测政策对经济的影响到天气预报，再到我们最初接触大部分科学的方式。我们要么认为这些决策非常简单，要么接受它们过于复杂以至我们无法解决，更不用说完全解释它们了。这就像AI的黑箱：人类的智慧能够针对一个给出的结论进行推理和论证，但无法解释我们得出一个特定结论的复杂、隐含的过程。<br><br>试想一下一对夫妻因某个明确的原因（例如，不忠）而离婚这个问题——在现实中，有许多完全看不见的、错综复杂的原因、影响和事件共同促成了这一结果。为什么这一对夫妇选择分手，而另一对类似情况的夫妇却没有?即使是处于这些关系中的人也无法完全解释这个问题。这是一个黑箱。<br><br>### AI的黑箱更多是一个特征，而不是一个bug<br><br>具有讽刺意味的是，与人类智能相比，人工智能实际上更加透明。与人类的思维不同，人工智能可以——也应该——被审问和被解释。例如检查和改进模型的能力，揭示深度神经网络中的知识差距，必须要构建的调试工具，以及通过脑机接口增强人类只能的潜在能力，等等，有许多技术可以帮助解释人工智能，而这些解释AI的方式无法用于解释人脑。在这个过程中，我们甚至可以更多地了解人类智能的运作方式。<br><br>也许批评者们担忧的真正原因不是我们无法“看到”AI的推理过程，而是当AI变得愈加强大时，人类的心智就变成了限制因素。他们担心的是，在未来，我们需要利用AI去理解AI。<br><br>在医疗领域以及其他领域，这意味着我们很快就会看到一个新类别的专业人士的出现，他们自己不必去做即时的决策，而是管理一个AI工人去做决策——就像商用飞机的驾驶员在恶劣的天气条件下使用自动驾驶仪降落一样。医生将不再“主导”初始诊断；相反，他们需要确保AI系统对患者的诊断是相关的和易于理解的，并监督AI在何时以及如何提供更多的说明和解释。未来的医生办公室很可能有多名计算机助理，包括医生方面的和病人方面的，以及来自外部的数据输入。<br><div align="center"><br><img src="http://p6ux47i4n.bkt.clouddn.com/AI-governance-lead.jpg/600.400" alt="aiinhealthy.jpg"><br></div><br>当这种情况成为现实时，显然，所谓的人工智能“黑箱”将更多是一种特征，而不是一个bug——因为它相比人类的大脑更能够理解和解释决策的过程。这并没有否定或忽视对AI进行监督的需求，只是说与其担心黑箱，我们更应该关注机会，从而更好地应对这样一个未来：AI不仅增强人类智能和人类直觉，而且甚至可以启发人之本质。<br><br>### 不要为了可解释性牺牲AI的能力<br><br>当前的AI系统可能会发生一些故障，例如使自动驾驶汽车遭遇事故，或在用于司法时对黑人判处相比白人更长的刑期。我们会知道这些，是因为AI已经在这些方面出现了错误。但是，这并不意味着我们应该坚持AI需要解释它在任何情况下如何做决策，包括欧盟的“一般数据保护条例”（GDPR）也如此要求。<br><div align="center"><br><img src="http://p6ux47i4n.bkt.clouddn.com/AIDRIVING.jpeg/600.400" alt="aiDRIVING.jpg"><br></div><br>要求可解释性听起来不错，但实现它可能需要让AI人为地变蠢。机器学习有如此强大的使用前景，缩减AI的能力可能意味着无法诊断疾病、无法发现气候变化的重要原因，等等。充分利用机器学习的能力意味着必须依赖那些现在无法向人类大脑解释的结果。<br><br>机器学习，特别是深度学习，可以将数据分析成数以千计的变量，将它们排列成非常复杂而敏感的加权关系数组，然后通过基于计算机的神经网络反复运行这些数组。要想理解这些运行的结果，例如为什么系统认为有73％的几率患上糖尿病，或者在象棋中走这步棋有84％的几率能导致最终胜利，这就需要理解这些成千上万的变量之间的关系，这些变量是通过大量的神经网络计算得出的。我们的大脑根本无法掌握这么多的信息。<br><div align="center"><br><img src="http://p6ux47i4n.bkt.clouddn.com/deep-learning-medicine-miccai.jpg/600.400" alt="aiinhealthy.jpg"><br></div> <p>可解释性是工具：我们用这些工具来达成目标。通过机器学习，可解释性能够帮助开发人员debug。可解释性也可以用来判断一个结果是否基于不应该计数的因素（例如性别，种族等，取决于具体情况）来评估责任。但是，我们可以通过其他方法来实现预期的效果，而不用约束机器学习系统的能力。</p><p>一个很有前景的工具是优化（optimization）。例如，在20世纪70年代石油危机期间，美国政府决定将限速降至55英里/时，从而优化高速公路。同样，政府也可以决定对自动驾驶汽车进行优化。</p><p>AI系统需要对针对某个目的的优化及其结果保持透明，特别是对我们希望它们支持的一些关键值保持透明。但是不一定要求算法是透明的。如果一个系统没有达到它的目标，就需要对它进行调优。如果达到了目标，可解释性就不是必要的。</p><p>通过将AI的可解释性问题视为优化问题，我们可以将争论集中在真正重要的问题上：我们想从一个系统中得到什么，我们愿意放弃什么来得到它？  </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;em&gt;概要：AI算法对人类生活的影响越来越大，但它们内部的运作往往是不透明的，人们对这种技术的工作方式也愈加感到担忧。&lt;/em&gt;&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://p6ux47i4n.bkt.clouddn.com/ai.jpg&quot; alt=&quot;ai.jpg&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;br&gt;AI算法对人类生活的影响越来越大，但它们内部的运作往往是不透明的，人们对这种技术的工作方式也愈加感到担忧。MIT科技评论曾经发表一篇题为“人工智能中的黑暗秘密”的文章，警告说：“没有人真正知道先进的机器学习算法是怎样工作的，而这恐将成为一大隐忧。”由于这种不确定性和缺乏问责制，纽约大学AI Now Institute的一份报告建议负责刑事司法、医疗保健、社会福利和教育的公共机构不应该使用AI技术。&lt;br&gt;
    
    </summary>
    
    
      <category term="AI" scheme="https://zhenfenghan.github.io/tags/AI/"/>
    
      <category term="黑箱" scheme="https://zhenfenghan.github.io/tags/%E9%BB%91%E7%AE%B1/"/>
    
  </entry>
  
  <entry>
    <title>信息茧房</title>
    <link href="https://zhenfenghan.github.io/2018/04/10/%E4%BF%A1%E6%81%AF%E8%8C%A7%E6%88%BF/"/>
    <id>https://zhenfenghan.github.io/2018/04/10/信息茧房/</id>
    <published>2018-04-10T06:01:11.000Z</published>
    <updated>2018-04-10T06:29:16.192Z</updated>
    
    <content type="html"><![CDATA[<h3 id="信息流"><a href="#信息流" class="headerlink" title="信息流"></a>信息流</h3><p><div align="center"><br><img src="http://p6ux47i4n.bkt.clouddn.com/feed.jpg/600" alt="Feed.jpg"><br></div><br>信息流充斥在我们生活中的每一个角落，如同河流一样哺育着每一个人。我们阅读的每一条新闻、看的每一段视频，一切通过信息流出现在我们眼前的东西，多多少少都受到了智能推荐的驱动。<br>　<br><a id="more"></a></p><h3 id="智能推荐如何才能了解用户？"><a href="#智能推荐如何才能了解用户？" class="headerlink" title="智能推荐如何才能了解用户？"></a>智能推荐如何才能了解用户？</h3><p><div align="center"><br><img src="http://p6ux47i4n.bkt.clouddn.com/passive.jpeg" alt="passive.jpg">　　<br></div><br><strong>主动与被动间的认知鸿沟。</strong>　　<br>其实智能推荐的行为逻辑很简单，那就是把适合的内容推荐给适合的用户。但在简单的行为逻辑中的，却是智能推荐的本质：内容和用户两方面的双向深度理解。</p><p>首先在对用户的理解上，很多平台都会陷入一个误区，那就是把用户的被动反应当成了主动索求。</p><p>比如很多资讯类推荐平台<strong>冷启动</strong>时，都会让用户选择自己感兴趣的话题，这一行为就已经把用户画像圈定在了平台自己设置的范围之内。实际这种理解用户的方式略有片面，即使不断挖掘也只能察觉到用户在阅读这一个场景中的状态，无法察觉用户在阅读中的喜好、无法察觉用户当下的需求。</p><p>这也就形成了信息流最严重的污名——信息茧房，智能推荐只会根据用户的兴趣爱好推荐内容，久而久之用户就会被自己关心的事物围绕，从而失去对外界的整体认知。尤其当低俗、猎奇、软色情这些刺激眼球的信息出现时，人们难免会因为下意识的好奇进行浏览，这一典型的被动反应将相关的标签加入了用户画像中，导致相关内容大量污染用户的信息流。</p><p>其实有时候信息茧房的形成并非内容出产者和平台故意灌输带有刺激性的内容给用户，而是一些信息流产品缺少获取用户主动索取行为的途径，犹如将用户放置入一个狭小的环境中，用户对环境产生的一点点反应都会在环境中形成反复的回声。可我们无法确定环境之外用户的主动行为，从而形成了巨大的认知鸿沟。</p><h3 id="信息茧房的负面影响"><a href="#信息茧房的负面影响" class="headerlink" title="信息茧房的负面影响"></a>信息茧房的负面影响</h3><p><div align="center"><br><img src="http://p6ux47i4n.bkt.clouddn.com/jianfang.jpg" alt="Cocoon.jpg">　　<br></div><br>客观看，“信息茧房”的产生在一定程度上顺应了当前媒体去中心化、裂变化、社交化的内容生产模式，体现了媒体主动迎合用户需求的趋势。但“信息茧房”带来的负面影响也不容忽视。　　</p><p>一方面，它<strong>加剧了网络群体的极化</strong>。在网络空间，网民通过血缘、地缘、学缘、业缘等关系产生分化和类聚，进而形成“信息茧房”。他们内部之间畅聊甚欢，但也慢慢减少了与外部其他群体之间的交流。群体成员之间接触到的信息、观点和看法基本上是一样的，群体同质化趋向日益显著。在这样的“信息茧房”内，人们容易将自己的偏见当作真理，将他人的合理观点拒之于千里之外。　　<br>　　<br>另一方面，它<strong>导致社会黏性降低</strong>。在“信息茧房”里，人们对信息的选择性输入不断增强。长期沉浸在自我话语体系中，排斥异己的观点和价值观，容易产生脱离社会的倾向，对小群体外的个人和社会漠不关心，导致整个社会黏性降低，形成社会共识、增强社会凝聚力日益困难。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;信息流&quot;&gt;&lt;a href=&quot;#信息流&quot; class=&quot;headerlink&quot; title=&quot;信息流&quot;&gt;&lt;/a&gt;信息流&lt;/h3&gt;&lt;p&gt;&lt;div align=&quot;center&quot;&gt;&lt;br&gt;&lt;img src=&quot;http://p6ux47i4n.bkt.clouddn.com/feed.jpg/600&quot; alt=&quot;Feed.jpg&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;br&gt;信息流充斥在我们生活中的每一个角落，如同河流一样哺育着每一个人。我们阅读的每一条新闻、看的每一段视频，一切通过信息流出现在我们眼前的东西，多多少少都受到了智能推荐的驱动。&lt;br&gt;　&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="推荐系统" scheme="https://zhenfenghan.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>PCA数据降维</title>
    <link href="https://zhenfenghan.github.io/2018/04/09/PCA%E6%95%B0%E6%8D%AE%E9%99%8D%E7%BB%B4/"/>
    <id>https://zhenfenghan.github.io/2018/04/09/PCA数据降维/</id>
    <published>2018-04-09T14:01:11.000Z</published>
    <updated>2018-04-10T01:17:11.628Z</updated>
    
    <content type="html"><![CDATA[<h2 id="降维的作用"><a href="#降维的作用" class="headerlink" title="降维的作用"></a><strong>降维的作用</strong></h2><ul><li>数据在低维下更容易处理、更容易使用；</li><li>相关特征，特别是重要特征更能在数据中明确的显示出来；如果只有两维或者三维的话，更便于可视化展示；</li><li>去除数据噪声</li><li>降低算法开销</li></ul><a id="more"></a><h2 id="降维通俗点的解释"><a href="#降维通俗点的解释" class="headerlink" title="降维通俗点的解释"></a><strong>降维通俗点的解释</strong></h2><p>一些高维度的数据，比如淘宝交易数据，为便于解释降维作用，我们在这假设有下单数，付款数，商品类别，售价四个维度，数据量上百万条，对于下单数和付款数，我们可以认为两者是线性相关的，即知道下单数，我们可以得到付款数，这里很明显这两个属性维度有冗余，去掉下单数，保留付款数，明显能再保证原有数据分布和信息的情况下有效简化数据，对于后面的模型学习会缩短不少时间和空间开销。这就是降维，当然并不是所有数据中都会有过于明显线性相关的属性维度，我们降维后最终的目标是各个属性维度之间线性无关。</p><h2 id="PCA降维步骤原理"><a href="#PCA降维步骤原理" class="headerlink" title="PCA降维步骤原理"></a><strong>PCA降维步骤原理</strong></h2><p>首先既然要度量那些是否存在相关的属性，我们就要用到协方差，这里不再赘述，协方差衡量的是2维属性间的相关性，对于n个维度的属性，就需要协方差矩阵，其对角线为各维度的方差。</p><p><strong>步骤：</strong></p><p> 设有m条n维数据。</p><p><strong>                      1）将原始数据按列组成n行m列矩阵X</strong></p><p><strong>                      2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值</strong></p><p><strong>                      3）求出协方差矩阵</strong></p><p><strong>                      4）求出协方差矩阵的特征值及对应的特征向量r</strong></p><p><strong>                      5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P</strong></p><p><strong>                      6）即为降维到k维后的数据</strong></p><h2 id="关于维数k的选择"><a href="#关于维数k的选择" class="headerlink" title="关于维数k的选择"></a><strong>关于维数k的选择</strong></h2><p>使用一个公式 ：   <strong><em>error = </em></strong><br><img src="http://p6ux47i4n.bkt.clouddn.com/pca_error.png" alt="PCA">，<br>表示压缩后的误差，m所有特征的个数，然后确定一个阈值x，比如0.01，选取一个K，使得error &lt; x则我们认为这个m可以接受，否则尝试其他.</p><h2 id="python中sklearn库的pca实现"><a href="#python中sklearn库的pca实现" class="headerlink" title="python中sklearn库的pca实现"></a><strong>python中sklearn库的pca实现</strong></h2><p><img src="http://p6ux47i4n.bkt.clouddn.com/pca.png" alt="pca"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;降维的作用&quot;&gt;&lt;a href=&quot;#降维的作用&quot; class=&quot;headerlink&quot; title=&quot;降维的作用&quot;&gt;&lt;/a&gt;&lt;strong&gt;降维的作用&lt;/strong&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;数据在低维下更容易处理、更容易使用；&lt;/li&gt;
&lt;li&gt;相关特征，特别是重要特征更能在数据中明确的显示出来；如果只有两维或者三维的话，更便于可视化展示；&lt;/li&gt;
&lt;li&gt;去除数据噪声&lt;/li&gt;
&lt;li&gt;降低算法开销&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="PCA" scheme="https://zhenfenghan.github.io/tags/PCA/"/>
    
      <category term="Python" scheme="https://zhenfenghan.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>梨花风起正清明</title>
    <link href="https://zhenfenghan.github.io/2018/04/05/%E6%B8%85%E6%98%8E/"/>
    <id>https://zhenfenghan.github.io/2018/04/05/清明/</id>
    <published>2018-04-05T14:01:11.000Z</published>
    <updated>2018-04-09T03:50:18.180Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一朝春醒-万物清明"><a href="#一朝春醒-万物清明" class="headerlink" title="一朝春醒 万物清明"></a>一朝春醒 万物清明</h2><div align="center"><br><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=139377&auto=0&height=66"></iframe><br></div> <div align="center"><br><a href="https://imgchr.com/i/CiU5uR" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2018/04/09/CiU5uR.jpg" alt="CiU5uR.jpg"></a><br></div>   <p><strong><em>“万物生长此时，皆清洁明净。故谓之清明。”</em></strong><br>在我国，很少有一个节日，像清明这样意蕴深厚而含混：风清景明，慎终追远，这是一个悲怆的日子；放歌踏青，追逐春天，这又是一个轻盈的日子。<strong>清明是唯一一个既是节气又是节日的日子。</strong>  </p><ul><li>为测试博客，特此附上有关清明的诗词。<a id="more"></a><div align="center"><br><a href="https://imgchr.com/i/CCD3jJ" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2018/04/06/CCD3jJ.jpg" alt="CCD3jJ.jpg"></a><br></div>   <h3 id="苏堤清明即事"><a href="#苏堤清明即事" class="headerlink" title="苏堤清明即事"></a><center><strong>苏堤清明即事</strong></center></h3></li></ul><p><center>作者：吴惟信 (宋)</center></p><p><center><strong><em>梨花风起正清明，游子寻春半出城。</em></strong></center></p><p><center>日暮笙歌收拾去，万株杨柳属流莺。</center></p><hr><h3 id="破阵子"><a href="#破阵子" class="headerlink" title="破阵子"></a><center><strong>破阵子</strong></center></h3><p><center>作者：晏殊 (宋)</center></p><p><center><strong><em>燕子来时新社，梨花落后清明。</em></strong></center></p><p><center>池上碧苔三四点，叶底黄鹂一两声，日长飞絮轻。</center></p><p><center>巧笑东邻女伴，采桑径里逢迎。</center></p><p><center><strong><em>疑怪昨宵春梦好，元是今朝斗草赢，笑从双脸生。</em></strong></center></p><hr><div align="center"><br><a href="https://imgchr.com/i/CCBJFf" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2018/04/06/CCBJFf.jpg" alt="CCBJFf.jpg"></a><br></div><h3 id="临安春雨初霁"><a href="#临安春雨初霁" class="headerlink" title="临安春雨初霁"></a><center><strong>临安春雨初霁</strong></center></h3><p><center>（宋）陆游</center></p><p><center>世味年来薄似纱，谁令骑马客京华？</center></p><p><center><strong><em>小楼一夜听春雨，深巷明朝卖杏花。</em></strong></center></p><p><center>矮纸斜行闲作草，晴窗细乳戏分茶。</center></p><p><center>素衣莫起风尘叹，犹及清明可到家。  </center></p><hr><h3 id="渔歌子"><a href="#渔歌子" class="headerlink" title="渔歌子"></a><center><strong>渔歌子</strong></center></h3><p><center>作者：魏承班 (唐)</center></p><p><center>柳如眉，云似发，鲛绡雾縠笼香雪。</center></p><p><center>梦魂惊，钟漏歇，窗外晓莺残月。</center></p><p><center><strong><em>几多情，无处说，落花飞絮清明节。</em></strong></center></p><p><center>少年郎，容易别，一去音书断绝。</center></p><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一朝春醒-万物清明&quot;&gt;&lt;a href=&quot;#一朝春醒-万物清明&quot; class=&quot;headerlink&quot; title=&quot;一朝春醒 万物清明&quot;&gt;&lt;/a&gt;一朝春醒 万物清明&lt;/h2&gt;&lt;div align=&quot;center&quot;&gt;&lt;br&gt;&lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=&quot;330&quot; height=&quot;86&quot; src=&quot;//music.163.com/outchain/player?type=2&amp;id=139377&amp;auto=0&amp;height=66&quot;&gt;&lt;/iframe&gt;&lt;br&gt;&lt;/div&gt; 


&lt;div align=&quot;center&quot;&gt;&lt;br&gt;&lt;a href=&quot;https://imgchr.com/i/CiU5uR&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;img src=&quot;https://s1.ax1x.com/2018/04/09/CiU5uR.jpg&quot; alt=&quot;CiU5uR.jpg&quot;&gt;&lt;/a&gt;&lt;br&gt;&lt;/div&gt;   

&lt;p&gt;&lt;strong&gt;&lt;em&gt;“万物生长此时，皆清洁明净。故谓之清明。”&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;在我国，很少有一个节日，像清明这样意蕴深厚而含混：风清景明，慎终追远，这是一个悲怆的日子；放歌踏青，追逐春天，这又是一个轻盈的日子。&lt;strong&gt;清明是唯一一个既是节气又是节日的日子。&lt;/strong&gt;  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;为测试博客，特此附上有关清明的诗词。&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
    
      <category term="清明" scheme="https://zhenfenghan.github.io/tags/%E6%B8%85%E6%98%8E/"/>
    
      <category term="随笔" scheme="https://zhenfenghan.github.io/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>Will&#39;s First Blog</title>
    <link href="https://zhenfenghan.github.io/2018/04/04/Will-s-first-blog/"/>
    <id>https://zhenfenghan.github.io/2018/04/04/Will-s-first-blog/</id>
    <published>2018-04-04T11:18:47.000Z</published>
    <updated>2018-04-06T14:12:29.531Z</updated>
    
    <content type="html"><![CDATA[<p>这是一篇测试文章，欢迎关注作者博客: <a href="http://willhan.xyz/" target="_blank" rel="noopener">http://willhan.xyz/</a>  或  <a href="https://zhenfenghan.github.io/">https://zhenfenghan.github.io/</a>  <strong>Thank you!</strong> ^_^<br><a id="more"></a><br><strong>Welcome to my little world</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是一篇测试文章，欢迎关注作者博客: &lt;a href=&quot;http://willhan.xyz/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://willhan.xyz/&lt;/a&gt;  或  &lt;a href=&quot;https://zhenfenghan.github.io/&quot;&gt;https://zhenfenghan.github.io/&lt;/a&gt;  &lt;strong&gt;Thank you!&lt;/strong&gt; ^_^&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="域名" scheme="https://zhenfenghan.github.io/tags/%E5%9F%9F%E5%90%8D/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://zhenfenghan.github.io/2018/04/04/hello-world/"/>
    <id>https://zhenfenghan.github.io/2018/04/04/hello-world/</id>
    <published>2018-04-04T10:54:02.690Z</published>
    <updated>2018-04-06T14:59:34.784Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a><br><a id="more"></a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;Quick-Start&quot;&gt;&lt;a href=&quot;#Quick-Start&quot; class=&quot;headerlink&quot; title=&quot;Quick Start&quot;&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;&lt;h3 id=&quot;Create-a-new-post&quot;&gt;&lt;a href=&quot;#Create-a-new-post&quot; class=&quot;headerlink&quot; title=&quot;Create a new post&quot;&gt;&lt;/a&gt;Create a new post&lt;/h3&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ hexo new &lt;span class=&quot;string&quot;&gt;&quot;My New Post&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;More info: &lt;a href=&quot;https://hexo.io/docs/writing.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Writing&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Hexo" scheme="https://zhenfenghan.github.io/tags/Hexo/"/>
    
  </entry>
  
</feed>
